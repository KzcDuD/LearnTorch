# Gradient Descent 梯度下降

## Explain

+ 先任意假設一組權重(W)，每一層的 output 就等於前一層的 input 乘以權重(W)
  即 y = sum(w * x)，這個過程就是 **Forward Propagation**。

+ 我們反推回去(*Backpropagation*)，用『梯度下降法』，逐步調整權重(W)，逼近最佳解，以達到『損失函數最小化』。

+ 不斷循環，直到損失的縮小已經不顯著了，我們就認定那一組權重是最佳解了。

---

1. 隨機初始化參數的值。
2. 計算目標函數對於這些參數的梯度。
3. 根據梯度的方向和學習率（一個控制更新步幅的超參數）來更新參數的值。
4. 重複步驟2和步驟3，直到達到收斂條件或達到一個事先設定的迭代次數。

### Example

[numpy example](./numpy_predict.py)

+ 從X , Y 逼近w 求出 `forward(x)` 預測值

```python
X = np.array([1,2,3,4], dtype=np.float32)`
Y = np.array([2,4,6,8], dtype =np.float32)`
```

[pytorch example](./pytorch_predict.py)

```python
backward() = gradient(X,Y,y_pred)
dw = w.grad
```

## 反向傳播演算法跟梯度下降的關係

+ 反向傳播演算法（Backpropagation Algorithm）是機器學習和深度學習中用於*訓練神經網絡*的一種重要技術，而梯度下降則是一種*最小化或最大化目標函數的優化算法*。這兩者之間存在密切的關係，因為反向傳播演算法是梯度下降法在神經網絡訓練中的具體應用。

+ 在深度學習中，神經網絡通常由許多層組成，每一層都包含多個神經元（或稱為節點）。這些神經元之間有權重（weights）和偏差（biases），而這些參數需要進行訓練以使得神經網絡能夠進行有效的學習和預測。

+ 反向傳播演算法是通過計算損失函數（Loss Function）對於神經網絡參數的梯度，然後根據梯度的方向來更新參數的值。這就是梯度下降的核心過程，只不過在反向傳播演算法中，梯度是由後向傳播的方式計算的。具體來說，反向傳播演算法的過程如下：

1. `正向傳播（Forward Propagation）`：將輸入數據通過神經網絡，計算預測值，並計算預測值和實際值之間的損失（誤差）。

1. `反向傳播（Backward Propagation）`：從輸出層開始，根據損失函數計算神經網絡參數（權重和偏差）的梯度。這是通過使用鏈式法則（Chain Rule）來將損失在每一層向前傳遞，計算每個參數對損失的貢獻。

1. `更新參數`：根據計算得到的梯度和梯度下降算法的規則（例如學習率），更新神經網絡的參數，使得損失函數值降低。

1. `重複迭代`：重複進行上述步驟，直到達到停止條件（例如達到固定的迭代次數或損失函數收斂）。

+ 反向傳播演算法的主要優勢在於它可以高效地計算神經網絡參數的梯度，而這在深度神經網絡中是非常重要的，因為這些網絡通常包含大量的參數。梯度下降算法則是一個廣泛應用的優化方法，它可以最小化（或最大化）目標函數，尋找合適的參數值，使得模型能夠更好地擬合訓練數據並泛化到測試數據。

+ 因此，反向傳播演算法是梯度下降法在神經網絡訓練中的具體應用，通過梯度下降法不斷更新神經網絡的參數，使得神經網絡能夠學習到有效的特徵表示，從而達到良好的預測性能。

## Referance

[ithome Gradient Descent](https://ithelp.ithome.com.tw/articles/10198147)

[梯度最佳解](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E6%95%B8%E5%AD%B8-%E4%B8%89-%E6%A2%AF%E5%BA%A6%E6%9C%80%E4%BD%B3%E8%A7%A3%E7%9B%B8%E9%97%9C%E7%AE%97%E6%B3%95-gradient-descent-optimization-algorithms-b61ed1478bd7)
