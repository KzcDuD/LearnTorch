# Active function

## List Function

+ Sigmoid Function
  + Typically in the last layer of a binary classification problem

+ TanH Function
  + Hiddem layer

+ ReLU Funcion
  + Normally use in hidden layer

+ Leaky ReLU Function ( only use API in torch.nn.functional )
  + Improved version of ReLU ,**when weigth no update** try to use Leaky ReLU

+ Softmax
  + Use in last layer ,in multi class classification problems

[Backpropagation with Softmax / Cross Entropy](https://medium.com/hoskiss-stand/backpropagation-with-softmax-cross-entropy-d60983b7b245)